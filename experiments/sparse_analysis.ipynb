{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n",
      "cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Loading model checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from MemSE.train_test_loop import test, test_mse_th, test_mse_sim\n",
    "from MemSE.model_load import load_memristor, find_existing\n",
    "from MemSE.dataset import get_dataloader\n",
    "from MemSE.nn import mse_gamma\n",
    "\n",
    "device = torch.device('cuda')\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print(device)\n",
    "\n",
    "bs = 4\n",
    "nb_batch = 100 / bs\n",
    "train_loader, valid_loader, test_loader, nclasses, input_shape = get_dataloader('CIFAR10', bs=bs)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "models_names = ['smallest_vgg', 'really_small_vgg']\n",
    "\n",
    "N = 128\n",
    "\n",
    "memse = load_memristor(models_names[0], nclasses, 'all', device, input_shape, 0.01, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(1018.5178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-4599.0479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(404.2074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(11585.6113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(172282.5312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-3402047.5000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(9819.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-673554.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(46880556., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-1.1322e+11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(571448.3125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-1.7408e+10, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.5693e+11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7716e+08, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.7716e+08, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.2068e+08, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.1465e+11, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.4371e+09, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.8/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.8/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.8/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.8/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/linuxbrew/.linuxbrew/Cellar/python@3.9/3.9.8/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_7652/3680760978.py\", line 6, in <module>\n",
      "    mu, ga, p_tot = memse.forward(batch)\n",
      "  File \"/home/sebastien/workspace/MemSE/MemSE/MemSE.py\", line 70, in forward\n",
      "    x, gamma, P_tot_i, gamma_shape = linear_layer_logic(s.weight, x, gamma, self.learnt_Gmax[i], self.quanter.Wmax[i], self.sigma, self.r, gamma_shape, compute_power=compute_power)\n",
      "  File \"/home/sebastien/workspace/MemSE/MemSE/mse_functions.py\", line 267, in linear_layer_logic\n",
      "    mu, gamma, gamma_shape = linear_layer_vec_batched(mu, gamma, W, sigma_c, r, gamma_shape=gamma_shape)\n",
      "  File \"/home/sebastien/workspace/MemSE/MemSE/mse_functions.py\", line 30, in linear_layer_vec_batched\n",
      "    new_gamma = oe.contract('ij,bjk,kl->bil', r ** 2 * G, gamma, G.T)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/opt_einsum/contract.py\", line 507, in contract\n",
      "    return _core_contract(operands, contraction_list, backend=backend, **einsum_kwargs)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/opt_einsum/contract.py\", line 573, in _core_contract\n",
      "    new_view = _tensordot(*tmp_operands, axes=(tuple(left_pos), tuple(right_pos)), backend=backend)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/opt_einsum/sharing.py\", line 131, in cached_tensordot\n",
      "    return tensordot(x, y, axes, backend=backend)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/opt_einsum/contract.py\", line 374, in _tensordot\n",
      "    return fn(x, y, axes=axes)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/opt_einsum/backends/torch.py\", line 54, in tensordot\n",
      "    return torch.tensordot(x, y, dims=axes)\n",
      "  File \"/home/sebastien/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/torch/functional.py\", line 1159, in tensordot\n",
      "    return _VF.tensordot(a, b, dims_a, dims_b)  # type: ignore[attr-defined]\n",
      " (Triggered internally at  ../torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 16]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7652/3680760978.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mga\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mga\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmemse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/BinaryConnectFaultyMemory/.env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 16]], which is output 0 of AsStridedBackward0, is at version 4; expected version 3 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "batch, _ = next(iter(train_loader))\n",
    "batch = batch.to(device)\n",
    "tar = memse.model(batch)\n",
    "batch.requires_grad_()\n",
    "memse.quant()\n",
    "mu, ga, p_tot = memse.forward(batch)\n",
    "loss = mse_gamma(tar, mu, ga)\n",
    "loss.mean().backward()\n",
    "memse.unquant()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1de2029d32b0e8ab558393913afb0dd151843c55642a0a449a3bb16dbda2cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
